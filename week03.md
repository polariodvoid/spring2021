
# Week 3 -- Classifiers (9 Feb)

## Assignment

Complete the following exercises before class #3 on Feb 9:

1. Create a visualization for entire Iris dataset that allows you to quickly distinguish separable features and species
    * Raschka has some code for doing something like this in cells 6 & 7 of his ch10.ipynb
    * [ch10.ipynb](https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch10/ch10.ipynb)
2. Use logistic regression with two features to model two classes that are not separable.
    * plot loss vs epoch to assess convergence of modeling training
    * plot decision regions along with the data
    * write your notebook so that it's easy to change species and features
3. Logistic regression in 1-D
    * Repeat #2 for 1 feature and 2 classes
    * Plot the best-fit model as a probability. See, for example: Figure 4.2 of "Introduction to Statistical Learning"

## Reading

* Rasckha & Mirjalili, Chapter 3: Tour of ML classifiers using scikit-learn
    * The rest of the chapter
    * Familiarize yourself with [ch03.ipynb](https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch03/ch03.ipynb) 
* [Section 5.05 of VanderPlas](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.05-Naive-Bayes.ipynb) -- github

## Study guide for Week #3 quiz

* Some quiz questions may related to the assignment
* Logistic regression as a conditional probability
* Stochastic vs batch gradient descent
* SVC vs logistic regression
* Naive Bayes
* Overfitting vs underfitting
* Decision trees
* KNN
* Curse of dimensionality
* Regularization
