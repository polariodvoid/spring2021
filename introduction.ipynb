{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"introduction.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPRrERWCICEtZsIiLjPs+/D"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"4vNtuciYLN_Y"},"source":["<table align=\"center\">\n","   <td align=\"center\"><a target=\"_blank\" href=\"https://colab.research.google.com/github/umbcdata602/spring2021/blob/master/introduction.ipynb\">\n","<img src=\"http://introtodeeplearning.com/images/colab/colab.png?v2.0\"  style=\"padding-bottom:5px;\" />Run in Google Colab</a></td>\n","</table>\n","\n","# Introduction\n","\n","* Artificial Intelligence (AI)\n","    * Subfield of computer science\n","    * Computer systems that perform tasks normally involving human intelligence\n","    * Includes algorithms that play chess and started beating human masters 40 years ago\n","    * Includes theory of artificial neurons dating back more than 60 years\n","* Machine learning (ML)\n","    * Subfield of AI\n","    * Self-learning algorithms use data to make predictions\n","        * ML models have adjustable parameters that are \"trained\" with data.\n","        * Trained models use new data to make predictions\n","* Deep Learning\n","    * Subfield of ML often involving big data\n","    * Neural networks can have many (millions, or more) trainable parameters\n","    * Massively parallel calculations require modern GPUs\n","    * [MIT 6.S191](http://introtodeeplearning.com/) lectures\n","\n","<img src=\"https://github.com/rasbt/python-machine-learning-book-3rd-edition/raw/master/ch01/images/01_01.png\" width=\"600\" />"]},{"cell_type":"markdown","metadata":{"id":"oDLq6jghD3h4"},"source":["# Statistical Learning\n","\n","* Statistical and mathematical foundations are centuries old\n","    * Bayes Theorem (1700s)\n","    * Calculus, linear algebra, optimization (1600s)\n","    * [Hastie & Tibshirani lectures](https://www.r-bloggers.com/in-depth-introduction-to-machine-learning-in-15-hours-of-expert-videos/) and applications in R\n","* Why now?\n","    * Data (Big Data)\n","    * Hardware (GPUs)\n","    * Software (Open Source)"]},{"cell_type":"markdown","metadata":{"id":"SlSc6InufDuh"},"source":["# Some applications\n","\n","* Handwritten text recognition (e.g., ZIP codes)\n","* Detecting tumors in images (brain scans, mammograms, skin photos, etc.)\n","* Spam detection\n","* Classifying news articles, social networks\n","* Voice and image recognition\n","* Chatbots and personal assistants (e.g., Siri)\n","* Text translation\n","* Genomic data analysis\n","* Robotics\n","* Self-driving cars\n","* Jeopardy (IBM Watson in 2011) & Go (AlphaGo beat Lee Sedol in 2016)\n","* And the list goes on...\n"]},{"cell_type":"markdown","metadata":{"id":"fvBefIKEErAC"},"source":["# Machine Learning\n","\n","* Three types\n","    * Supervised, \n","    * Unsupervised \n","    * Reinforcement\n","* Image credits: [*Python Machine Learning, 3rd Edition* (2019) by Raschka & Mirjalili](https://github.com/rasbt/python-machine-learning-book-3rd-edition/raw/master/ch01/images/01_01.png)\n","    * In general, \\<img\\> tags point to original sources.\n"]},{"cell_type":"markdown","metadata":{"id":"91_oi4Hzixlf"},"source":["\n","# Supervised learning -- classification\n","\n","<img src=\"https://github.com/rasbt/python-machine-learning-book-3rd-edition/raw/master/ch01/images/01_02.png\" width=\"600\"/>\n","\n","* Qualitative predictions (labels & features)\n","* Binary classes, e.g., Yes or No, Malignant or Benign\n","* Multiple classes, e.g., Iris species: Setosa, Versicolor or Virginica\n","\n","\n","<img src=\"https://github.com/rasbt/python-machine-learning-book-3rd-edition/raw/master/ch01/images/01_03.png\" width=\"400\"/>\n","\n","* Mathematically, this binary classification model can be written\n","\\begin{align*}\n","  y =  \\left\\lbrace\n","  \\begin{array}{r@{}l}\n","    \\mathrm{yes}, & z \\geq 0 \\\\\n","    \\mathrm{no}, & z < 0\n","  \\end{array}\n","  \\right.\n","\\end{align*}\n","where\n","$$\n","z = w_0 + w_1 x_1 + w_2 x_2\n","$$\n","* The class $y$ can take on a finite set of qualitative values.\n","* The dependent variable $z$ is a function of $x_1$ and $x_2$.\n","* The independent variables $x_1$ and $x_2$ are called \"features\".\n","* $w_0$, $w_1$, and $w_2$ are \"trainable\" parameters\n","    * $w_1$ and $w_2$ are called weights. \n","    * $w_0$ is called a bias term.\n","* The dashed line corresponds to the equation $z = 0$.\n","* Trainable parameters are \"learned\" from data or observations.\n","    * Training data have known values of $y$ and $x_i$\n","\n"]},{"cell_type":"markdown","metadata":{"id":"NVBUnBjkv9Kf"},"source":["# Fisher's Iris dataset\n","\n","* This is a classic dataset for ML classification.\n","* [Iris flower dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set) --  wikipedia.org\n","\n","<img src=\"https://github.com/rasbt/python-machine-learning-book-3rd-edition/raw/master/ch01/images/01_08.png\" width=\"600\"/>"]},{"cell_type":"markdown","metadata":{"id":"ZDUD2QL_DcdL"},"source":["\n","# Supervised Learning -- regression\n","\n","* Quantitative dependent variable(s) take on any value\n","* Mathematically, the regression problem in the figure is\n","$$\n","y = f(x) + \\epsilon\n","$$\n","* $f(x)$, the model of interests, is a function of $x$\n","* $\\epsilon$ represents random noise (e.g., measurement error)\n","* If the model is a linear function of $x$, then\n","$$\n","f(x) = w_0 + w_1 * x\n","$$\n","* You can perform linear regression with a nonlinear model such as\n","$$\n","f(x) = w_0 + w_1 x + w_2 x^2\n","$$\n","\n","\n","\n","<img src=\"https://github.com/rasbt/python-machine-learning-book-3rd-edition/raw/master/ch01/images/01_04.png\" width=\"400\"/>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Sjg2jpFW6eA1"},"source":["# Logistic Regression & Linear Regression\n","* You may have started seeing notational similaries between classification and regression.\n","* \"Statistical Learning\" perspective clarifies the relationship...\n","    * Weights are not known *a priori* and must be estimated (learned) from data.\n","    * Learning requires an objective function that measures \"goodness of fit\" between model and data\n","    * Least-squares (linear) regression involves minimizing the sum of squared errors between $y$ and $f$\n","    * Classification (logistic regression) involves minimizing a quantity known as \"cross entropy\"\n","    * In fact, both techniques maximize a statistical measure of \"likelihood\" given a set of observations.\n","* Reference: [Chapter 8 of \"Elements of Statistical Learning, 2nd Edition\"](https://web.stanford.edu/~hastie/ElemStatLearn/) (2009) by Hastie, Tibshirani & Friedman\n","    * 12th printing (2017) is freely available on the web\n"]},{"cell_type":"markdown","metadata":{"id":"wQ3KCacxK1gP"},"source":["# Synonyms\n","\n","* **Training example**: row in a table representing the dataset; an observation, record, instance, or sample.\n","* **Feature**: $x$, column in a data table; predictor, variable, input, attribute, or covariate.\n","* **Training**: Model fitting, parameter estimation.\n","* **Target**: $y$, outcome, output, response variable, dependent variable, (class) label, and ground truth.\n","* **Loss function**: cost function, objective function, error function used to train a model."]},{"cell_type":"markdown","metadata":{"id":"FsP_PFKU0PTv"},"source":["# Unsupervised learning\n","\n","* Clustering -- discovering structure/patterns\n","\n","<img src=\"https://github.com/rasbt/python-machine-learning-book-3rd-edition/raw/master/ch01/images/01_06.png\" width=\"400\"/>\n","\n","* Dimensionality reduction\n","\n","<img src=\"https://github.com/rasbt/python-machine-learning-book-3rd-edition/raw/master/ch01/images/01_07.png\" width=\"600\"/>"]},{"cell_type":"markdown","metadata":{"id":"WuE7q4RRr79V"},"source":["# Reinforcement learning\n","\n","<img src=\"https://github.com/rasbt/python-machine-learning-book-3rd-edition/raw/master/ch01/images/01_05.png\" width=\"600\"/>"]},{"cell_type":"markdown","metadata":{"id":"LOWUIqM5Kc5o"},"source":["# Typical workflow\n","\n","\n","<img src=\"https://github.com/rasbt/python-machine-learning-book-3rd-edition/raw/master/ch01/images/01_09.png\" width=\"600\"/>\n"]},{"cell_type":"markdown","metadata":{"id":"VFmHkpAqb1XQ"},"source":["# Our tools\n","\n","* Python\n","    * R, JavaScript, C, C++, Julia, Matlab, SAS, Fortran\n","    * [tiobe index](https://www.tiobe.com/tiobe-index/)\n","* [Google Colab](https://colab.research.google.com/notebooks/intro.ipynb)\n","    * Check out their [introductory video](https://www.youtube.com/watch?v=inN8seMm7UI) featuring Jake VanderPlas\n","* Python science libraries\n","    * numpy, pandas, matplotlib\n","    * scikit-learn\n","    * [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/) (2016) by VanderPlas\n","    * Tensorflow & Keras -- [tensorflow.org](www.tensorflow.org)\n","\n","# Our texts\n","\n","* [Python Machine Learning, 3rd Edition (2019) by Raschka & Mirjalili](https://github.com/rasbt/python-machine-learning-book-3rd-edition) -- github\n","* [Introduction to Statistical Learning](https://www.statlearning.com/) (2013) by James, Witten, Hastie and Tibshirani -- statlearning.com\n","* and others listed in the syllabus"]}]}